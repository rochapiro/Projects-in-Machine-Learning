{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification Using the Stanford SST Sentiment Dataset"
      ],
      "metadata": {
        "id": "Md-lX3nuB9Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Get data in and set up X_train, X_test, y_train objects\n"
      ],
      "metadata": {
        "id": "Y2w2ATh5DgBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github link: "
      ],
      "metadata": {
        "id": "W6NBrVAQCC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install aimodelshare==0.0.189"
      ],
      "metadata": {
        "id": "g-uPrWG4CFcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Restart the runtime after the installation"
      ],
      "metadata": {
        "id": "2TS2x3_5D0js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data in from the package \n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syOopO5oDYxs",
        "outputId": "e6429d4b-412c-4b79-ef81-488d75babaa2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion of the data set and the importance of text classification models:\n",
        " \n",
        "The data to be used for the models below consist of different movie reviews which are classified into negative or possitive based on their commenatry on the movie. Building a model that can acuratley predict the sentiment behing commentary can be a very powerfull tool that could be leveraged for any type of textual feedback. Sucha model could have many implementations acorss a wide range of fields that rely on commenatry or feedback such as publi policy, mental health, and entertainment. Having the ability to create models that predict the sentiment behind commentary can a powerfull tool for stakeholder to meassure the public response to a particular project and make changes as needed within a resonable time, depending on the problem being tackled.   "
      ],
      "metadata": {
        "id": "JetdgGqAHwDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# ohe encode Y data\n",
        "y_train = pd.get_dummies(y_train_labels)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLMwa3BcET4W",
        "outputId": "ae93c509-a4f8-43a5-acab-cfffb99e8f1d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The Rock is destined to be the 21st Century 's...\n",
              "1    The gorgeously elaborate continuation of `` Th...\n",
              "2    Singer/composer Bryan Adams contributes a slew...\n",
              "3                 Yet the act is still charming here .\n",
              "4    Whether or not you 're enlightened by any of D...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.   Preprocess data using keras tokenizer / Write and Save Preprocessor function"
      ],
      "metadata": {
        "id": "Q5gyvffWE0BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This preprocessor function makes use of the tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length\n",
        "def preprocessor(data, maxlen=40, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8sixV2_EwbK",
        "outputId": "d0186efe-5692-4bf7-d4f0-f00990c96ffa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 40)\n",
            "(1821, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the preprocesor in a local file \n",
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2iqWoyh3e80",
        "outputId": "a4b27546-f8b4-496c-cd11-4c28bcaeb603"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your preprocessor is now saved to 'preprocessor.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Fit models on preprocessed data and save preprocessor function and model \n"
      ],
      "metadata": {
        "id": "kGfFKNQyE9Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 1:"
      ],
      "metadata": {
        "id": "SFvKvCNn6JLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 16, input_length=40))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnPle1OgE4bR",
        "outputId": "8080ee15-aff3-4f7b-83cb-4285ad955c09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 640)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 1282      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,288\n",
            "Trainable params: 161,288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 2s 6ms/step - loss: 0.6676 - acc: 0.6149 - val_loss: 0.8357 - val_acc: 0.1488\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 1s 4ms/step - loss: 0.6415 - acc: 0.6152 - val_loss: 0.7898 - val_acc: 0.2189\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.5792 - acc: 0.6969 - val_loss: 0.7845 - val_acc: 0.3613\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.4864 - acc: 0.8219 - val_loss: 0.7203 - val_acc: 0.5224\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.3862 - acc: 0.8940 - val_loss: 0.7083 - val_acc: 0.5679\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.2978 - acc: 0.9306 - val_loss: 0.6615 - val_acc: 0.6496\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.2254 - acc: 0.9552 - val_loss: 0.6080 - val_acc: 0.6900\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.1672 - acc: 0.9686 - val_loss: 0.5949 - val_acc: 0.7059\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.1231 - acc: 0.9780 - val_loss: 0.6530 - val_acc: 0.6900\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.0906 - acc: 0.9852 - val_loss: 0.6898 - val_acc: 0.6879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the model abive to and \".onnx\" file "
      ],
      "metadata": {
        "id": "W8EZ6wz_4joM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "xtwfTbQoHh9S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from aimodelshare.aws import set_credentials \n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o6DqD6W4jAZ",
        "outputId": "9c8b89bb-4871-4b8b-a302-45e9732f668c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate Competition\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "metadata": {
        "id": "5LSYPX8G5QHn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submitting model 1: "
      ],
      "metadata": {
        "id": "Mj56PuCj5btV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #-- Generate predicted y values (Model 1)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpi51dmJ5XKt",
        "outputId": "452d914b-f73a-4906-86de-dd41637333da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 1ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 399\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2: "
      ],
      "metadata": {
        "id": "N4ZVxv5j6Mv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the hyperparametres\n",
        "\n",
        "vocab_size = 10000  # Limit the vocabulary size to the top 10,000 words\n",
        "maxlen = 40        # Set the maximum sequence length\n",
        "embedding_dim = 32  # Dimension of the word embeddings\n",
        "lstm_units = 36    # Number of LSTM units"
      ],
      "metadata": {
        "id": "oW5ZAHVU7bY3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Bidirectional, TimeDistributed, BatchNormalization, Dropout\n",
        "\n",
        "model2 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2)),\n",
        "    TimeDistributed(Dense(lstm_units, activation='relu')),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model2.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHuKBOOx6OAR",
        "outputId": "30cf1103-19f2-494f-b666-1387b1b9e455"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 27s 101ms/step - loss: 0.6258 - acc: 0.6463 - val_loss: 0.6966 - val_acc: 0.2854\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.4488 - acc: 0.8002 - val_loss: 0.6714 - val_acc: 0.5629\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 20s 118ms/step - loss: 0.3306 - acc: 0.8696 - val_loss: 0.6058 - val_acc: 0.8085\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 23s 135ms/step - loss: 0.2339 - acc: 0.9084 - val_loss: 0.9504 - val_acc: 0.4964\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 14s 82ms/step - loss: 0.1930 - acc: 0.9321 - val_loss: 0.6249 - val_acc: 0.7399\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 16s 90ms/step - loss: 0.1492 - acc: 0.9483 - val_loss: 0.6698 - val_acc: 0.7876\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.1149 - acc: 0.9588 - val_loss: 1.1531 - val_acc: 0.7464\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.0973 - acc: 0.9691 - val_loss: 1.3279 - val_acc: 0.7218\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 15s 88ms/step - loss: 0.0799 - acc: 0.9727 - val_loss: 1.6037 - val_acc: 0.6395\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 14s 80ms/step - loss: 0.0643 - acc: 0.9805 - val_loss: 1.1258 - val_acc: 0.7066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "ZTv4OIn162EM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submitting model 2[texto del enlace](https://): "
      ],
      "metadata": {
        "id": "TcGL9m2W9B99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model 2: \n",
        "\n",
        "#-- Generate predicted y values (Model 2)\n",
        "prediction_column_index=model2.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAQFxGE48yXq",
        "outputId": "099fcf79-c309-40ca-b833-f92469caab38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 5s 32ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 401\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 3: "
      ],
      "metadata": {
        "id": "B5o347uF9hzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "model3 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model3.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFMCU4ge84Nl",
        "outputId": "b731dd1a-0bd3-4374-8f6a-acc8701c8624"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 42s 21ms/step - loss: 0.6789 - acc: 0.5988 - val_loss: 0.7247 - val_acc: 0.1488\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 3s 20ms/step - loss: 0.5640 - acc: 0.6833 - val_loss: 0.6490 - val_acc: 0.8114\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 4s 23ms/step - loss: 0.3656 - acc: 0.8539 - val_loss: 0.4455 - val_acc: 0.8591\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1935 - acc: 0.9306 - val_loss: 0.5717 - val_acc: 0.7854\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 5s 27ms/step - loss: 0.0908 - acc: 0.9697 - val_loss: 0.9869 - val_acc: 0.7536\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 3s 20ms/step - loss: 0.0401 - acc: 0.9879 - val_loss: 1.3802 - val_acc: 0.7529\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 3s 20ms/step - loss: 0.0352 - acc: 0.9886 - val_loss: 3.3901 - val_acc: 0.5918\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 4s 25ms/step - loss: 0.0426 - acc: 0.9874 - val_loss: 2.2900 - val_acc: 0.6445\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.0385 - acc: 0.9897 - val_loss: 2.5324 - val_acc: 0.6409\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 4s 26ms/step - loss: 0.0242 - acc: 0.9924 - val_loss: 2.5171 - val_acc: 0.6777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model3, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model3.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "GL-Goi8E-yse"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model 3: \n",
        "\n",
        "#-- Generate predicted y values (Model 3)\n",
        "prediction_column_index=model3.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model3.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGvc7Mbs-4es",
        "outputId": "82cbfb53-7e02-4654-adf2-9de0fa64b095"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 4ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 418\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 4:"
      ],
      "metadata": {
        "id": "dUYjhgKvC-yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloaded the pre-trained word vector document from here: https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "S8f3sxjyGnE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "I2k-7u65K3E8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the downloaded pre-trained vector of words\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_file = '/content/drive/MyDrive/glove/glove.6B.100d.txt'  \n",
        "glove_embeddings = load_glove_embeddings(glove_file)\n"
      ],
      "metadata": {
        "id": "zVcZqpfBC_zn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "labels =pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad the sequences\n",
        "maxlen = 40\n",
        "x_data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "integer_labels = label_encoder.fit_transform(labels)\n",
        "y_data = to_categorical(integer_labels)\n"
      ],
      "metadata": {
        "id": "DZUm4t1DKgPV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the embedding matrix \n",
        "def create_embedding_matrix(glove_embeddings, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 100  # Update the embedding dimension based on the GloVe embeddings\n",
        "embedding_matrix = create_embedding_matrix(glove_embeddings, word_index, embedding_dim)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CR5szt_bEEW7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# running the model based on the prior steps \n",
        "vocab_size = len(word_index) + 1\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "maxlen = 40  \n",
        "\n",
        "model4 = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, embeddings_initializer=Constant(embedding_matrix),\n",
        "              input_length=maxlen, trainable=False),  # Set trainable to False to freeze the embeddings\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model4.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBdJkqaJeZQ",
        "outputId": "19b4a60e-4ba6-4419-e952-e465f5d20766"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_15 (Embedding)    (None, 40, 100)           1383600   \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 128)               512128    \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,895,986\n",
            "Trainable params: 512,386\n",
            "Non-trainable params: 1,383,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 40  \n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "X_test, y_train_labels = x_data, y_data\n",
        "x_val, y_val = x_data, y_data\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 40\n",
        "history = model4.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjbEzA58MNAO",
        "outputId": "b3783ca6-73cd-4c0e-aae8-967dc3d7724d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 2s 8ms/step - loss: 0.6505 - accuracy: 0.6295 - val_loss: 0.6802 - val_accuracy: 0.6019\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.5375 - accuracy: 0.7169 - val_loss: 0.6499 - val_accuracy: 0.6438\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.4585 - accuracy: 0.7764 - val_loss: 0.7867 - val_accuracy: 0.5455\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.3900 - accuracy: 0.8266 - val_loss: 0.8668 - val_accuracy: 0.5499\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.3324 - accuracy: 0.8528 - val_loss: 0.7683 - val_accuracy: 0.6293\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.2686 - accuracy: 0.8884 - val_loss: 0.8884 - val_accuracy: 0.5882\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.2333 - accuracy: 0.9044 - val_loss: 0.8518 - val_accuracy: 0.6257\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.1840 - accuracy: 0.9279 - val_loss: 0.9098 - val_accuracy: 0.6301\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 2s 10ms/step - loss: 0.1624 - accuracy: 0.9364 - val_loss: 1.1906 - val_accuracy: 0.5621\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.1410 - accuracy: 0.9498 - val_loss: 1.4549 - val_accuracy: 0.5043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model4, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model4.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "OmEr368-SPyj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-- Generate predicted y values \n",
        "prediction_column_index=model4.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 4 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNRaWo17YC9v",
        "outputId": "06b5d4ba-5ebe-4172-eaee-5882ba259b16"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 4ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 419\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the model performance, right of the strat we can see that the CONV1D layers helped a lot with the classification problemm when compared withe LSTM layers and the plain vanilla model using only dense layers at the beguining. There is a lot of information to be extracted via CNN techniques when applied to text classification. The transfer learning model performed very well too when compared to the LSTM. However I did use a pre-trained vector of word; link is above for it. The number of filter for the models with CONV1D layers had an efect on the performance of the model; as more filter were able to extract more meaningful relationships. The max lenth in the preprocesor function is annother important hyper paramter as smaller lengths were able to extract more information. Making the LSTM layer bidirectional also helped with the models classification performance. Kernel size also seemed to help the overal models ferformance for CONV1D layers. "
      ],
      "metadata": {
        "id": "bS--hTj1EFEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post discussion Models: "
      ],
      "metadata": {
        "id": "x8NPf48QTx1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 1: CONV1D\n"
      ],
      "metadata": {
        "id": "2CDCQGduT2Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_t = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model1_t.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model1_t.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbWhUyX1PLRV",
        "outputId": "617d4a6a-cd5e-4aab-9588-acfa2f742760"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 10s 45ms/step - loss: 0.7123 - acc: 0.5878 - val_loss: 0.7180 - val_acc: 0.1488\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 12s 69ms/step - loss: 0.6299 - acc: 0.6494 - val_loss: 0.6568 - val_acc: 0.8208\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 8s 48ms/step - loss: 0.4483 - acc: 0.8009 - val_loss: 0.6035 - val_acc: 0.7283\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 11s 62ms/step - loss: 0.3015 - acc: 0.8876 - val_loss: 0.5899 - val_acc: 0.7283\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 9s 54ms/step - loss: 0.1791 - acc: 0.9362 - val_loss: 0.8799 - val_acc: 0.7471\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 9s 54ms/step - loss: 0.1050 - acc: 0.9671 - val_loss: 1.6226 - val_acc: 0.6958\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 12s 67ms/step - loss: 0.0791 - acc: 0.9749 - val_loss: 1.7562 - val_acc: 0.7045\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 16s 90ms/step - loss: 0.0784 - acc: 0.9751 - val_loss: 3.7267 - val_acc: 0.5860\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 10s 56ms/step - loss: 0.0672 - acc: 0.9823 - val_loss: 5.7491 - val_acc: 0.5332\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 9s 52ms/step - loss: 0.0745 - acc: 0.9792 - val_loss: 4.4753 - val_acc: 0.5816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model1_t, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model1_t.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "XiGgUk4oVCjg"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model: \n",
        "\n",
        "#-- Generate predicted y values \n",
        "prediction_column_index=model1_t.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model1_t.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TTZy9BcWN-v",
        "outputId": "558a8d61-e48a-4f9b-e217-b46774584535"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 11ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 421\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2: LSTM "
      ],
      "metadata": {
        "id": "rW-EN7j-YxAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the hyperparametres\n",
        "\n",
        "vocab_size = 10000  # Limit the vocabulary size to the top 10,000 words\n",
        "maxlen = 20        # Set the maximum sequence length\n",
        "embedding_dim = 24  # Dimension of the word embeddings\n",
        "lstm_units = 15    # Number of LSTM units"
      ],
      "metadata": {
        "id": "BJu4NFgwZC15"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(data, maxlen=20, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK7TgpbBZsGW",
        "outputId": "7b5fffe9-a551-4f9e-d7d6-a82bfa8757a9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 20)\n",
            "(1821, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_t = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2)),\n",
        "    TimeDistributed(Dense(lstm_units, activation='relu')),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "model2_t.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model2_t.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEeYFb2rWZUp",
        "outputId": "4a7194cc-a691-48c8-f66a-d34866f77dcc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 18s 62ms/step - loss: 0.6475 - acc: 0.6232 - val_loss: 0.6852 - val_acc: 0.8483\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 5s 28ms/step - loss: 0.5016 - acc: 0.7583 - val_loss: 0.6234 - val_acc: 0.8273\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 5s 28ms/step - loss: 0.3782 - acc: 0.8385 - val_loss: 0.5926 - val_acc: 0.7355\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 8s 47ms/step - loss: 0.2963 - acc: 0.8831 - val_loss: 0.5259 - val_acc: 0.7666\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.2240 - acc: 0.9167 - val_loss: 0.9899 - val_acc: 0.7052\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 5s 27ms/step - loss: 0.1698 - acc: 0.9386 - val_loss: 1.0045 - val_acc: 0.6749\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 7s 41ms/step - loss: 0.1436 - acc: 0.9474 - val_loss: 0.8626 - val_acc: 0.7370\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1016 - acc: 0.9671 - val_loss: 1.1806 - val_acc: 0.7218\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 5s 28ms/step - loss: 0.0959 - acc: 0.9711 - val_loss: 1.5028 - val_acc: 0.6618\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.0704 - acc: 0.9762 - val_loss: 0.8484 - val_acc: 0.8049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model2_t, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2_t.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "bZhnKLKUZw6Y"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model: \n",
        "#-- Generate predicted y values \n",
        "prediction_column_index=model2_t.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model2_t.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzzMxk7jaVYg",
        "outputId": "48340f22-7b05-448c-c6ae-bef655ac5a1f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 3s 14ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 423\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 3: CONV1D"
      ],
      "metadata": {
        "id": "vShqHmitak6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessor tokenizes words and makes sure all documents have the same length\n",
        "def preprocessor(data, maxlen=40, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZExO34Laj-j",
        "outputId": "94f1710e-ee70-437f-c28a-ed016ec2c561"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 40)\n",
            "(1821, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3_t = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=40),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model3_t.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model1_t.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GRvjw3Vaybu",
        "outputId": "ab76c231-67ba-4bad-f75a-618f0f850418"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 11s 63ms/step - loss: 0.0483 - acc: 0.9881 - val_loss: 2.8655 - val_acc: 0.7146\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 8s 48ms/step - loss: 0.0413 - acc: 0.9902 - val_loss: 2.3315 - val_acc: 0.7493\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 10s 58ms/step - loss: 0.0422 - acc: 0.9924 - val_loss: 1.9455 - val_acc: 0.7645\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 9s 54ms/step - loss: 0.0299 - acc: 0.9942 - val_loss: 4.0874 - val_acc: 0.6676\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 9s 53ms/step - loss: 0.0348 - acc: 0.9897 - val_loss: 4.0682 - val_acc: 0.6727\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 10s 58ms/step - loss: 0.0311 - acc: 0.9922 - val_loss: 3.7036 - val_acc: 0.7110\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 8s 47ms/step - loss: 0.0260 - acc: 0.9940 - val_loss: 5.1418 - val_acc: 0.6749\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 11s 64ms/step - loss: 0.0400 - acc: 0.9926 - val_loss: 5.2805 - val_acc: 0.6431\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 7s 43ms/step - loss: 0.0369 - acc: 0.9902 - val_loss: 3.0168 - val_acc: 0.6763\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 12s 69ms/step - loss: 0.0272 - acc: 0.9940 - val_loss: 7.8958 - val_acc: 0.6669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model3_t, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model3_t.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "oPVGG_IUcEXy"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model: \n",
        "#-- Generate predicted y values \n",
        "prediction_column_index=model3_t.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model3_t.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_0AfW61bHPY",
        "outputId": "06090192-75d6-4ddd-d703-5da511391cee"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 13ms/step\n",
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 428\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Discussion:\n",
        "\n",
        "After commenting and going over what hyperparameters to changes in order to improve the overall performance of the models, we can divide the best models using CONV1D layers and LSTM layers into separte categories for hypertuning.In the case of models using CONVD1D layers, the two parameters that stood out the most where the kernel and filter size for the CONV1D layer, as increasing these helped the model classification power. In regrd to the LSTM layers, making them bidirectional improved the performance of the model when compared to ones that did not use it. The other relevant parameter for LSTM layers prooved to be the Maximum length argument as smaller numbers performed better. "
      ],
      "metadata": {
        "id": "JVTqEWhedYLN"
      }
    }
  ]
}